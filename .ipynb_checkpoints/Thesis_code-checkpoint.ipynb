{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63fe255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandarallel\n",
      "  Using cached pandarallel-1.5.8.tar.gz (12 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dill>=0.3.1\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting pandas>=1\n",
      "  Using cached pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/umerfarooq/.local/lib/python3.8/site-packages (from pandas>=1->pandarallel) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/umerfarooq/.local/lib/python3.8/site-packages (from pandas>=1->pandarallel) (1.22.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.14.0)\n",
      "Building wheels for collected packages: pandarallel\n",
      "  Building wheel for pandarallel (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandarallel: filename=pandarallel-1.5.8-py3-none-any.whl size=16296 sha256=e971406e4795f04e2e036becd476a7012df33009f1e0e12e804546dc5b7e7e0a\n",
      "  Stored in directory: /home/umerfarooq/.cache/pip/wheels/ab/57/dc/1b789cd518579394a0e276d148d4fb0c2e5ea426aa4329acea\n",
      "Successfully built pandarallel\n",
      "Installing collected packages: pytz, dill, pandas, pandarallel\n",
      "Successfully installed dill-0.3.4 pandarallel-1.5.8 pandas-1.4.1 pytz-2021.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f1cac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create newfile cities.csv and add username and flines in csv file\n",
    "import pandas as pd\n",
    "col_list = [\"username\", \"flines\"]\n",
    "#pd.read_csv(file,) \n",
    "df = pd.read_csv(r\"gcj2009.csv\", usecols=col_list)\n",
    "#print col of dataframe\n",
    "#            print(df[\"username\"])\n",
    "#            print(df[\"flines\"])\n",
    "#write csv file\n",
    "df.to_csv(r'cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22ad356b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#remove punctuation from cities files\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "data = pd.read_csv(\"cities.csv\")\n",
    "\n",
    "#defining the function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return punctuationfree\n",
    "\n",
    "\n",
    "#storing the puntuation free text\n",
    "flines_dataframe=pd.DataFrame({\"username\":[data['username'][i] for i in range(0,len(data))],\"flines\":[remove_punctuation(str(data['flines'][i])) for i in range(0,len(data))]})\n",
    "flines_dataframe.to_csv(\"punctuation_free.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71970a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "#Extra code for tokenization that we attempted\n",
    "#for i in data.itertuples():\n",
    "    #nltk.word_tokenize(data[\"clean_msg\"][2])\n",
    "    #nltk.tokenize.WhitespaceTokenizer().tokenize(data[\"clean_msg\"][2])\n",
    "    #nltk.tokenize.WhitespaceTokenizer().tokenize(data[\"clean_msg\"][2])\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tokenization\n",
    "import csv\n",
    "from nltk import word_tokenize \n",
    "\n",
    "data=pd.read_csv(\"punctuation_free.csv\")\n",
    "dict1={\"username\":[],\"tokens\":[]}\n",
    "for i in range(0,len(data)):\n",
    "    dict1[\"username\"].append(str(data[\"username\"][i]))\n",
    "    dict1[\"tokens\"].append(word_tokenize(str(data[\"flines\"][i]).replace(\"\\n\",\" \").replace(\"\\t\",\" \")))\n",
    "    \n",
    "token_dataframe=pd.DataFrame(dict1)\n",
    "token_dataframe.to_csv(\"tokens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f17dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in reader:\n",
    "    clean_data = row[\"clean_msg\"]\n",
    "    print(\"flines: %s\" % clean_data)\n",
    "    tokens = word_tokenize(clean_data)\n",
    "    print(tokens)\n",
    "    #storing the puntuation free text\n",
    "import spacy\n",
    "from pandarallel import pandarallel\n",
    "spacy.prefer_gpu()\n",
    "pandarallel.initialize(progress_bar=True)    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#df['tokens_msg'] = df['clean_msg'].parallel_apply(lambda x: nlp(x))\n",
    "#csvfile['tokens_msg'].to_csv('D:\\Thesis work\\Dataset\\gcj-dataset-master\\tokens.csv')\n",
    "import nltk\n",
    "#storing the tokenization text\n",
    "#from pandarallel import pandarallel\n",
    "#pandarallel.initialize(progress_bar=True)\n",
    "#data['tokens_msg'] = csvfile['clean_msg'].parallel_apply(lambda x: nltk.word_tokenize(x))\n",
    "#data['tokens_msg'].to_csv('D:\\Thesis work\\Dataset\\gcj-dataset-master\\tokens.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc5c56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354d7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"token.csv\")\n",
    "bagOfWordsA=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f0f9a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sys': 1, 'sys.1': 1, 'sys.2': 1, \"'usrbinenv'\": 1, \" 'python'\": 1, \" 'import'\": 1, \" 'sys'\": 1, \" 'math'\": 1, \" 'def'\": 1, \" 'changeBaseFromTennumbase'\": 1, \" 'digits'\": 1, \" 'while'\": 1, \" 'num'\": 1, \" '0'\": 1, \" 'digitsinsert0'\": 1, \" 'strnum'\": 1, \" 'base'\": 1, \" 'num'.1\": 1, \" 'base'.1\": 1, \" 'return'\": 1, \" 'joindigits'\": 1, \" 'def'.1\": 1, \" 'isHappynumbase'\": 1, \" 'thisNum'\": 1, \" 'changeBaseFromTennumbase'.1\": 1, \" 'numsPassed'\": 1, \" 'thisNum'.1\": 1, \" 'while'.1\": 1, \" 'thisNum'.2\": 1, \" '1'\": 1, \" 'digits'.1\": 1, \" 'mapintthisNum'\": 1, \" 'sqDigits'\": 1, \" 'maplambda'\": 1, \" 'x'\": 1, \" 'powx2'\": 1, \" 'digits'.2\": 1, \" 'thisNum'.3\": 1, \" 'sumsqDigits'\": 1, \" 'thisNum'.4\": 1, \" 'changeBaseFromTenthisNumbase'\": 1, \" 'if'\": 1, \" 'thisNum'.5\": 1, \" 'in'\": 1, \" 'numsPassed'.1\": 1, \" 'return'.1\": 1, \" 'False'\": 1, \" 'else'\": 1, \" 'numsPassedappendthisNum'\": 1, \" 'return'.2\": 1, \" 'True'\": 1, \" 'n'\": 1, \" 'intsysstdinreadlinestrip'\": 1, \" 'for'\": 1, \" 'i'\": 1, \" 'in'.1\": 1, \" 'rangen'\": 1, \" 'bases'\": 1, \" 'mapintsysstdinreadlinestripsplit'\": 1, \" 'res'\": 1, \" '0'.1\": 1, \" 'for'.1\": 1, \" 'j'\": 1, \" 'in'.2\": 1, \" 'range210000000'\": 1, \" 'found'\": 1, \" 'True'.1\": 1, \" 'for'.2\": 1, \" 'base'.2\": 1, \" 'in'.3\": 1, 'Unnamed: 70': 1}\n"
     ]
    }
   ],
   "source": [
    "numOfWordsA = dict.fromkeys(bagOfWordsA, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "print(numOfWordsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c633533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9148e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6d055e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tfA \u001b[38;5;241m=\u001b[39m \u001b[43mcomputeTF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumOfWordsA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbagOfWordsA\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mcomputeTF\u001b[0;34m(wordDict, bagOfWords)\u001b[0m\n\u001b[1;32m      3\u001b[0m bagOfWordsCount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(bagOfWords)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, count \u001b[38;5;129;01min\u001b[39;00m wordDict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 5\u001b[0m     tfDict[word] \u001b[38;5;241m=\u001b[39m \u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbagOfWordsCount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tfDict\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c312406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cff4e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8464b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7af0b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "dataf = pd.DataFrame([tfidfA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c6b00198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  clean_msg\n",
       "0         0.0        0.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9025981b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11064/2328544659.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocumentA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdenselist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1844\u001b[0m         \"\"\"\n\u001b[0;32m   1845\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1846\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1847\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([documentA])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "dataf = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ea6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce84db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"tokens.csv\")\n",
    "l1=[data[\"tokens\"][i] for i in range(0,len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac6fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    " corpus = l1\n",
    "# [\n",
    "#       'this',\"is\",'the','first','document',\n",
    "#       'this document is the second document',\n",
    "#       'and this is the third one',\n",
    "#       'is this the first document',\n",
    "#  ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924b175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662f0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_all_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbafb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Vocabulary,idf_of_vocabulary=fit(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idf_of_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ddb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(Vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(idf_of_vocabulary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224785b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
